{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, random, pickle\n",
    "sys.path.append('../app')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from policy import HighLevelPolicy\n",
    "from virtual_game import VirtualGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    policy = Policy(load_states=True)\n",
    "    \n",
    "    config = Config()\n",
    "    config.blue_team_size = 4\n",
    "    config.red_team_size = 4\n",
    "    \n",
    "    q_state_actions = get_state_actions(policy)\n",
    "    print('%d state actions:\\n%s' % (len(state_actions), '\\n'.join(list(state_actions.items())[:5])))\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        q, x, desc = get_random_policy(q_state_actions, policy)\n",
    "\n",
    "        policy.q = q\n",
    "        \n",
    "        game = VirtualGame(config)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            for player_map_idx in range(len(game.the_map.players)):\n",
    "                action = policy.get_high_level_action(player_map_idx, the_map)\n",
    "                \n",
    "                reward, winner = game.step(player_map_idx, action)\n",
    "                if winner:\n",
    "                    \n",
    "    \n",
    "    \n",
    "def get_state_actions(policy):\n",
    "    state_actions = {}\n",
    "    \n",
    "    for state_code in policy.high_level_state_codes:\n",
    "        hls_str = get_hls(state_code, policy)\n",
    "        state_actions[hls_str]=[]\n",
    "        \n",
    "        for i,act in enumerate(policy.high_level_actions):\n",
    "            if 'self_incapacitated' in hls and not act=='wait':\n",
    "                continue\n",
    "            if act=='go_opponent_flag_carrier' and not 'team_flag_in_play' in hls:\n",
    "                continue\n",
    "            elif act=='gaurd_teammate_flag_carrier':\n",
    "                if 'self_has_flag' in hls or not 'opponents_flag_in_play' in hls:\n",
    "                    continue\n",
    "                    \n",
    "            state_actions[hls_str].append(i)\n",
    "            \n",
    "    return state_actions\n",
    "\n",
    "\n",
    "def get_hls(state_code, policy):\n",
    "    hls = []\n",
    "    for i,s in enumerate(policy.high_level_states):\n",
    "        if state_code[i]:\n",
    "            hls.append(s)\n",
    "    hls_str = ', '.join(hls) if hls else 'none'\n",
    "    return hls_str\n",
    "\n",
    "\n",
    "def get_random_policy(state_actions, policy):\n",
    "    policy_desc = []\n",
    "    x = []\n",
    "    q = np.zeros_like(policy.q)\n",
    "    \n",
    "    for state_idx, state_code in enumerate(policy.high_level_state_codes):\n",
    "        hls_str = get_hls(state_code, policy)\n",
    "        action_idx = random.choice(state_actions[hls_str])\n",
    "        q[state_idx, action_idx] = 1.0\n",
    "        policy_desc.append('%s : %s' % (hls_str, policy.high_level_actions[action_idx]))\n",
    "        x.append(action_idx)\n",
    "        \n",
    "    return q, np.array(x), policy_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_lgb(x_train, y_train, x_test, y_test, feats=[]):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    lr, n_estimators = get_best_lr_nestimators(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    gbm = lgb.LGBMClassifier(\n",
    "        learning_rate = lr, \n",
    "        metric = 'l1', \n",
    "        n_estimators = n_estimators)\n",
    "\n",
    "    gbm.fit(x_train, y_train,\n",
    "            eval_set=[(x_test, y_test)],\n",
    "            eval_metric=['auc', 'binary_logloss'], early_stopping_rounds=5)\n",
    "    \n",
    "    report_results(gbm, x_test, y_test, feats)\n",
    "    \n",
    "    return gbm\n",
    "    \n",
    "    \n",
    "def get_best_lr_nestimators(x_train, y_train, x_test, y_test):\n",
    "    estimator = lgb.LGBMClassifier(\n",
    "        learning_rate = 0.125, \n",
    "        metric = 'l1', \n",
    "        n_estimators = 20,\n",
    "        num_leaves = 38)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [x for x in range(24,40,2)],\n",
    "        'learning_rate': [0.10, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]}\n",
    "    \n",
    "    gridsearch = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "    gridsearch.fit(x_train, y_train,\n",
    "            eval_set = [(x_test, y_test)],\n",
    "            eval_metric = ['auc', 'binary_logloss'],\n",
    "            early_stopping_rounds = 5)\n",
    "    \n",
    "    return gridsearch.best_params_['learning_rate'], gridsearch.best_params_['n_estimators']\n",
    "    \n",
    "\n",
    "def report_results(gbm, x_test, y_test, feats):\n",
    "    if feats:\n",
    "        print('Feature Importance:')\n",
    "        for i in reversed(np.argsort(gbm.feature_importances_).tolist()):\n",
    "            print('\\t%s\\t%d' % (feats[i], gbm.feature_importances_[i]))\n",
    "        \n",
    "    y_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration_)\n",
    "    y_pred_prob = gbm.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    print('\\n\\n', str(classification_report(y_test, y_pred, digits=4)))\n",
    "    print('\\n\\nTN FP\\nFN TP\\n', confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('\\n\\nAUC ROC: ', str(roc_auc_score(y_test, y_pred_prob)))\n",
    "    \n",
    "\n",
    "def save_model(lgbm_model, path='lgbm.model'):\n",
    "    lgbm_model.booster_.save_model(path)\n",
    "    \n",
    "    \n",
    "def load_model(path='lgbm.model'):\n",
    "    return lgb.Booster(model_file=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = train_test_lgb(x_train, y_train, x_test, y_test, feat_names)\n",
    "save_model(lgbm_model, 'lgbm_arr_syn_10000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_results(gbm, x_test, y_test, feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, feat_names, data, an_pairs_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    clf = LogisticRegression().fit(x_train, y_train)\n",
    "    \n",
    "    print('Feature coefficients:')\n",
    "    wts = clf.coef_.squeeze()\n",
    "    for i,feat in enumerate(feat_names):\n",
    "        print(wts[i], feat)\n",
    "\n",
    "    preds = clf.predict(x_test)\n",
    "    probs = clf.predict_proba(x_test)\n",
    "\n",
    "    cor=0\n",
    "    nm = 'logistic_regression_error_records.txt'\n",
    "    with open(nm, 'w', encoding='utf-8') as o:\n",
    "        for i in range(preds.shape[0]):\n",
    "            trg = y_test[i]\n",
    "            pred = preds[i]\n",
    "            prob = probs[i][pred]\n",
    "            src_rec, trg_rec, _, _ = data[i]\n",
    "            src_an, trg_an, details = an_pairs_test[i]\n",
    "            \n",
    "            if trg==pred:\n",
    "                cor+=1\n",
    "            else:\n",
    "                o.write('Target: %d, Pred: %d, Prob: %.4f\\nSource:\\n%s\\n\\nTarget:\\n%s\\n\\nSim Details:\\n%s\\n\\n\\n' % (trg, \n",
    "                         pred, prob, str(src_rec), str(trg_rec), str(details)))\n",
    "    print(cor/preds.shape[0])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(x_train, y_train, x_test, y_test, feat_names, data, an_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
